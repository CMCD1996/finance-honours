% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
% !BIB TS-program = biber
% !BIB program = biber

\documentclass[12pt]{article}

%%% PAGE DIMENSIONS
\usepackage[margin=2.54cm]{geometry}
\geometry{a4paper} 
% \usepackage{caption}
% \usepackage{subcaption}
\usepackage{graphicx} % For better graphics
\usepackage{pdfpages} % To insert pdfs into the library
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{siunitx}

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{amsmath} % for better maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage[framed,numbered]{matlab-prettifier} % enable inserting matlab code.
\usepackage[parfill]{parskip}
%\addtolength{\jot}{1em}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{color}
\usepackage{listings}

% Create Listing Colours
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\usepackage{multicol}
\usepackage{float}

% References
\usepackage[backend=biber,style = apa]{biblatex}
\bibliography{sources}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\setlength{\headheight}{15pt}
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{University of Auckland}\chead{Finance 788}\rhead{Connor McDowall}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% Hyperlinking 
\usepackage{hyperref}
\begin{document}
\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	
	\center
	
	%------------------------------------------------
	%	Headings
	%------------------------------------------------
	
	\textsc{\LARGE }\\[1.5cm] % Main heading such as the name of your university/college
	
	\textsc{\Large University of Auckland\\Department of Accounting \& Finance}\\[0.5cm] % Major heading such as course name
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\HRule\\[0.5cm]
	
	{\huge\bfseries FINANCE 788: Research Essay}\\[0.4cm] % Title of your document
	
	\HRule\\[0.5cm]
	
	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
	
	{\large\textit{Author: Connor McDowall \\Supervisor: Dr Paul Geertsema}}\\
	
	%------------------------------------------------
	%	Date
	%------------------------------------------------
	
	\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	
	{\large\today} % Date, change the \today to a set date if you want to be precise
	 
	%----------------------------------------------------------------------------------------
	
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}
\newpage
\section*{Abstract}
\newpage
\section*{Acknowledgements}
\begin{center}
	\textbf{Paul Geertsema}
\end{center}
\newpage
\section*{Declaration of Contribution}
\newpage
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
\section{Introduction}
\newpage
\section{Literature Review}\label{LR}
\textbf{Insert Literature Review}
\subsection{History of Asset Pricing Theory}

\subsubsection{Optimisation Methodologies}
Convexity is an important concept in optimisation 
\subsubsection{Machine Learning in Financial Contexts}
A couple of recent publications highlight the increased application of machine learning algorithms in financial contexts.
\cite{corporate-culture}
Gu et al (\citeyear{eapvml}) explore the comparative use of machine learning in empirical asset pricing.

\section{Research Intent}
\textbf{Insert Research Intent}
\section{Theory}
\subsection{Return predictability}
Return predictability underlies asset pricing theory. 
\textbf{Insert}
\subsection{Modelling, loss, and optimisation}
We summarize the theory surrounding predictive modelling, loss functions, and optimisation algorithms.
These functions train models by comparing predictions to realized observations using optimisation algorithms to minimize the loss function.
We examine a linear model as our predictive model (\ref{lm}).
Mean square error (\ref{mse}) and Gradient Descent (GD) are basic examples of a loss function and optimisation algorithm, respectively.
\begin{align}
	\hat{y} &= mx_i + b \label{lm}\\
	f(y,(mx_{i} + b)) &= \frac{1}{n} \sum_{i=1}^{n}(y_i - (mx_{i} + b))^{2} \label{mse}
\end{align}
Firstly, gradient descent takes the partial derivatives of the loss function, with the respect to the parameters in our predictive model.
In our example, equations \ref{pdmsem} and \ref{pdmseb} are the partial derivatives for the mean square error loss function.
\begin{align}
	\frac{\partial f(y,(mx_{i} + b)}{ \partial m} &= \frac{1}{n} \sum_{i=1}^{n}-2x_{i}(y_i - (mx_{i} + b))^{2} \label{pdmsem}\\
	\frac{\partial f(y,(mx_{i} + b)}{ \partial b} &= \frac{1}{n} \sum_{i=1}^{n}-2(y_i - (mx_{i} + b))^{2} \label{pdmseb}
\end{align}
Secondly, the algorithm explores epochs, using a learning rate to update parameters to move in the opposite directions of the partial derivatives until settling in a local minima.
This extrema is the optimisation of the loss function, quantifying the accuracy of the predicative model.
Ordinary Least Squares (OLS) regressions is an extension of the linear model prevalent in asset pricing.
\subsection{Ordinary Least Squares (OLS)}\label{ols}
The OLS regression is the most prominent statistical model in asset pricing theory.
Rosenfeld \citeyear{olsmf} contributes an OLS summary.
The composition of the true OLS model includes four components.
Firstly, \textbf{X}, an n x k matrix of k independent variables for n observations.
Secondly, \textbf{y}, an n x 1 vector of observation on the dependent variable.
Thirdly, \textbf{$\epsilon$}, an n x 1 vector of unexplained error.
Lastly, $\theta$, a k x 1 vector of parameters to be estimated.
\begin{align}
	y &= X\theta + \epsilon
\end{align}
\subsubsection{Criteria for estimation}
The criteria to obtain the parameter estimate ($\hat{\theta}$) relies on the minimisation of the sum of squared residuals (\ref{ssr}).
We highlight the observed residuals (e) are distinct from unexplained disturbances ($\epsilon$).
Equation \ref{res} derives residuals by taking the difference between observations based on parameter estimates.
\begin{align}
	\sum & e_i^2  \label{ssr}\\
	e = y & - X \hat{\theta} \label{res}
\end{align}
Expanding the quadratic $e^{T}e$ after substituting in equation \ref{res} leads to the alternative expression of the sum of squared residuals in equation \ref{ssrm}.
Minimizing the sum of square residuals requires taking the partial derivative of equation \ref{ssrm} with respect to the estimated parameters (equation) using matrix differentiation (\ref{ssrmd}).
It is imperative X has full rank where all vectors in the matrix are linearly independent, validating both the presence of a positive definite matrix and minimum.
\begin{align}
	e^{T}e &= y^{T}y - 2\hat{\theta}^{T}X^{T}y + \hat{\theta}^{T}X^{T}\hat{\theta}X \label{ssrm}\\
	\frac{\partial e^{T}e}{\partial \hat{\theta}} &= - 2X^{T}y + 2X^{T}X\hat{\theta} =0 \label{ssrmd}
\end{align}
We find the expression for the Ordinary Least Squares (OLS) estimator (\ref{OLSD}) after rearranging equation \ref{ssrmd} to normal form, utilizing inverse matrices to form identity matrices, and simplifying.
\begin{align}
	2X^{T}X\hat{\theta} &= 2X^{T}y \\
	(X^{T}X)^{-1}(X^{T}X)\hat{\theta} &= (X^{T}X)^{-1}X^{T}y \\
	I\hat{\theta} &= (X^{T}X)^{-1}X^{T}y \\
	\hat{\theta} &= (X^{T}X)^{-1}(X^{T}y) \label{OLSD}\\
\end{align}
Therefore, we can use the OLS estimator to make predictions with OLS (\ref{OLS}).
\begin{align*}
	\hat{y} &= X^{T} \hat{\theta} \label{OLS}
\end{align*}
\subsubsection{Properties of OLS Estimators}
There are six key properties in addition to the satisfaction in minimizing the summation of squared residuals.
\begin{enumerate}
	\item The residuals are uncorrelated with the observed values of X i.e., $X^{T}e=0$.
	\item The sum of the residuals is zero i.e., $\sum e_i=0$.
	\item The sample mean of the residuals is zero i.e., $\bar{e} = \frac{\sum e_i}{n} = 0$.
	\item The regression hyperplane passes through the means of observed values i.e., $\frac{e} = \frac{y - X\theta}{n} = 0$. Since $\bar{e} = 0$ assumed, it is implied $\bar{y}=\bar{x}\bar{\theta}$.
	\item The residuals are uncorrelated with the predicted y i.e., $\hat{y} = X\hat{\theta}$, $\hat{y}^{T}e = (X\hat{\beta})^{T}e = b^{T}X^{T}e = 0$ 
	\item The mean of $\hat{y}$ for the sample will equal the mean of the y.
\end{enumerate}
\subsubsection{The Gauss-Markov Theorem}
However, OLS makes Gauss-Markov assumptions about the true model to make inferences regarding $\beta$ from $\hat{\beta}$.
The intention of the Gauss-Markov Theorem, conditional on the below assumptions, states the OLS estimator is the best linear, unbiased, and efficient estimator: 
\begin{align}
	y &= x\beta + \epsilon\\
	E[\epsilon|X] &= 0 \label{gma3}\\
	E(\epsilon \epsilon^{T}|X) &= \Omega = \sigma^{2}I \label{gma4}\\
	\epsilon &| X ~ N[0,\sigma^{T}I] \text{ (hypothesis testing)}
\end{align}
\begin{itemize}
	\item X is an n x k matrix of full rank
	\item X must be generated randomly, or fixed, by a mechanism uncorrelated to disturbances.
\end{itemize}
Equation \ref{gma3} implies $E(y) = X\beta$ as no observations of the independent variables convey any information about the expected values of the disturbances.
Equation \ref{gma4} captures homoskedasticity and no autocorrelation assumptions.
Additionally,
The theory underlying Ordinary Least Squares informs the common practice in minimising of the sum of least squares when evaluating prediction performance.
The mathematical tractability, in accordance with the aforementioned assumption, frame our thinking surrounding the derivation of custom loss functions.
\subsubsection{Weaknesses in OLS: Return Predictability}
\textbf{Include examples on the minimisation of sum of the square errors does not contribute to this subset}
\subsection{Portfolio Formation: Hedge Portfolios}
Our formation of hedge portfolios rely on monotonic functions.
These functions both preserve or reverse a given ordered set.
We rank the cross-sections of portfolio returns using variations in monotonic functions to assign weights and form hedge portfolios.
\begin{align}
	R(y_{i,t})
\end{align}
The ranking function (R($y_{i,t}$)) and thresholds (u,v) form subsets of long and short portfolios.
\begin{align}
	L &= \{ y_{i,t} | R(y_{i,t})\geq u\}\\
	S &= \{ y_{i,t} | R(y_{i,t})\leq v\}\\
	0 & < u < 1\\
	0 & < v < 1\\
	u & > v
\end{align}
These truth sets inform the construction of time-series hedge portfolios. 
The first set of time-series hedge portfolio equations assumes equal weighting in long and short portfolios through dividing each subset (L,S) by their cardinality. 
\begin{align}
	H_t &= \frac{1}{|L|}\sum_{i\epsilon L} y_{i,t} - \frac{1}{|S|}\sum_{i\epsilon S} y_{i,t}\\
\end{align}
Our aim is to re-configure the loss function to maximise returns 
Subsequently, this enables the derivation of objective functions ex-post transaction costs.
\subsection{Optimisation of Hedge Portfolios}
Table \ref{hpt} provides a simple 
	\begin{table}[H]
		\centering
		\begin{tabular}{||c|c|c|c||}
			\hline
			Variable & Description & $ MSE(y,\hat{y}) $ & $ HP(y,\hat{y}) $\\ [0.5ex]
			\hline
			&&&\\
			$\theta$ & Est/Train& $ \hat{\theta}_{MSE}$& $ \hat{\theta}_{HP}$ \\ [0.5ex]
			\hline
			&&&\\
			$\lambda$ & Validation & $\hat{\lambda}_{MSE}$ & $\hat{\lambda}_{HP}$\\ [1.0ex]
			\hline
		\end{tabular}
	\caption{Objective (MSE: Mean Square Error, HP: Hedge Portfolio)}
	\label{hpt}
\end{table}
\newpage

\section{Data}
\textbf{Expand: Dataset implies, use this dataset (\cite{jensen2021there})}

\section{Methodology}
\textbf{Adapt for the context of this research essay}
\subsection{Project organisation}
GOCPI adopted Data Science best practice, as described by Wilson et al \cite{J:10}. Although these practices are
mostly reserved for data science projects, their principles are suitable for product development and version control. All data and
results were saved regularly and reproducible. The retention of data in all forms received high levels of attention. Project files were synched
continuously to Google Drive \cite{Google_Drive}. Git \cite{Git} was used to manage version control for GOCPI's source code, data, documentation and results.
Git stores a complete history of versions using Git hashes. These hashes are strings unique to each state of the
publicly available GOCPI repository\footnote[1]{https://github.com/CMCD1996/GOCPI}. Git hashes enabled the discretisation of GOCPI's development over time,
enabling the accessibility and recollection of all previous states given a unique git hash. This functionality
enabled reproducibility, error correction and the ability to revert to previous models.

\subsubsection{Version Control}\label{Version Control}
Git, hosted by GitHub, provided a comprehensive set of version control technologies. These technologies provided a range of benefits.
Firstly, Git is excellent at providing and supporting collaborative functionalities. The master version of a project is accessible for all
who have access to the repository. Each contributor could create custom copies of branches through pull requests on the master branch. Contributors
could commit changes to custom branches and push these changes to the master branch through push requests. The product manager could review these push requests,
approving suitable requests to integrate changes to the master branch. Collaborative efforts were possible with
commit messages describing the contributions from each contributor. This project had one contributor. Git ensured the histories of code, work and authors are stored.
The descriptive nature of the commit log ensured an accurate journal is kept.

\subsubsection{Folder Structure}
GOCPI maintained the file folder structure recommended in Wilson et al \cite{J:10}. 
Project organisation was paramount as the modelling of energy systems involves integrating a range of optimisation models, data files and documents. 
Wilson et al's recommendations were appropriate as data science projects require similar organisational rigor. 
Subsequently, file management and structure was most efficient and comprehensive. \textbf{GOCPI} is the root directory of this project and contains several sub directories: bin, data, doc, src and results.  
The \textbf{bin} sub directory contained external scripts and compiled programmes related to the GOCPI project. 
The \textbf{data} sub directory contained all raw data associated with the project. 
This data included energy statistics, energy balance datasets, partitioned geographies, standardised optimisation models and TIMES modelling frameworks.
The \textbf{doc} sub directory stored GOCPI's user guides, academic resources, research reports and project deliverables.
The \textbf{results} sub directory contained the output from optimisation simulations and processed data to display on dashboards and websites to inform investment and policy decisions.
The \textbf{src} sub directory stores the source code for preparing raw data, partitioning sets of geographies with varying granularities and the GOCPI python package available to download using PyPI\footnote[2]{https://pypi.org/} and install using pip\footnote[3]{https://pypi.org/project/pip/}.
All files were continuously backed up using Google Drive.

\subsubsection{Python}\label{python}
Python 3.7 was the primary coding language for the GOCPI project. 
GOCPI's objective is to enable any user to design and model their own energy system to inform investment and policy decisions.
The intention is to empower users to discuss energy investment and policy decisions made by public and private parties.
Additionally, GOCPI intends to reduce misinformation regarding energy policies and help assess the feasibility of meeting the International Energy Agency's Sustainable Development Scenario \cite{IEA_WEM}.
Python is omnipresent, widespread in software development. 
Python's language design makes the language highly productive and simple to use. 
Python can hand off computationally straining tasks to C/C++ and has first-class integration capabilities with these two languages.
The language also has a very active and supportive community \cite{Python_Features}.
In addition, Python is the most popular coding language on the planet defined by the PYPL PopularitY of Programming Language Index. 
As at August 2020, Python had 31.59\% of all language tutorial search instances on Google \cite{PYPL_Pop}.
Python has many useful packages for creating the GOCPI package such as NumPy, Scikit-learn, os, csv and Pandas. 
Programming is quick due to Python's dynamic nature.
The language is also open-source with no cost.
Subsequently, Python was the best language to ensure the GOCPI model is accessible for many users to use and extend.

\subsubsection{Package Management}
The Anaconda package management platform for Python \cite{Anaconda} was the chosen coding environment.
Anaconda is a well defined, free platform with known versions of python packages such as matplotlib, numpy and pip.
The use of this environment ensured both reproducibility and consistency across infrastructure.
Although this project required no collaboration, the use of Anaconda will inform future developers on how to manage collaborative processes, especially for packages which are less well-maintained. Anaconda allows you to create custom environments which was necessary for creating scalable linear optimization problems to express energy systems.
Pip is Python's default package manager and is included in the Anaconda package. 
Pip was used to install and update packages for python not available on Anaconda such as twine and the custom GOCPI package developed for this project.

\subsubsection{Excel}
It is important users are comfortable with using the GOCPI model. 
Energy modelling can be quite complex. The modelling process must be transparent to inform users how to build their own models.
Excel is ubiquitous across academic and professional communities.
Excel's omnipotence makes the software well-suited for describing the components of the GNU Mathprog energy system model.
The \textbf{GOCPI OseMOSYS Structure.xlsx} file describes the sets, paramaters, constraints and objective function of a scalable energy system model.
The User may toggle statement sets, parameters and constraints to adjust the complexity of the model. 
The model file was imported to a text file.
However, data related to these energy systems was stored using Python dictionaries, lists and NumPy arrays.
This Python formulation was later transcribed to a text file.
Excel is best for two dimensional variables or data stored in Codd-Boyce relational databases \cite{CBNF}.
The majority of parameters in energy systems were three or more dimensions. 
Therefore, Excel was not suitable to store these parameters.
Python dictionaries, lists and NumPy arrays were preferred alternatives.

\subsubsection{IBM ILOG CPLEX Optimization}
The OseMOSYS methodology (see \ref{OseMOSYS}) translates energy systems into linear programming problems. 
A solver was required to optimise these user-defined energy systems.
The IBM ILOG Optimization Studio \cite{IBM_ILOG}, more commonly known as CPLEX, was chosen to be this solver.
CPLEX solves very large linear programming problems using the Barrier Interior-point method \cite{IPM} or primal/dual variants of the Simplex Method \cite{Simplex}.
GOCPI's user-defined energy systems could be scaled up to model very large systems, creating large linear programming problems.

The IBM ILOG CPLEX Optimization Studio has an interface with the Python language based on a C programming interface.
Subsequently, Python APIs were available to run the CPLEX solver when installed either locally or on a cloud service.
The python packages are \textbf{cplex} and \textbf{docplex}. The cplex package contains classes for accessing CPLEX for the Python programming language. 
The Cplex class is the most important class in this package as provides methods for creating, modifying, querying, or solving optimisation problems.
Docplex also enables the formulation of new linear programmes where one creates the model, defines the decision variables, sets the constraints and expresses the objective function.
The user uses docplex to solve the linear programme on a local solver. 
Alternatively, the model can be solved on a private cloud using Decision Optimisation on Cloud service through the provision of a service url and personal API key.
The CPLEX Python APIs were most attractive as provided the user with a powerful commercial solver in an accessible format.

There is a caveat to the use of the CPLEX solver. The IBM ILOG CPLEX Optimization Studio is commercial by nature and requires a license to use.
Fortunately IBM have the IBM Academic Initiative \cite{IBM_AI}, granting students access to commercial software for free.
This commercial nature creates accessibility issues for users who are not enrolled at an academic institution or can afford to pay for the software.
Accessibility issues caused by the need for commercial solvers must be addressed to enable the distribution of the GOCPI product.

\subsubsection{IBM Watson Machine Learning Service}
The IBM CPLEX Optimisation Cplex python API is suitable for smaller models that can be solved locally.
As the model increases in complexity, the docplex Python API did enable the ability to solve larger linear programmes.
Unfortunately, IBM phased out the docplex Python API by incorporating the Decision Optimisation on Cloud services into the IBM Watson Machine Learning cloud services \cite{IBM_WML}.
This change occurred during September 2020.
This service uses IBM Cloud to access assets through credentials, create model deployments in IBM's servers and execute jobs to solve models.
The model deployments must be Python-based models with jobs specifying a payloads containing input data and output formats.

\subsubsection{PyPI}\label{PyPI}
PyPI\footnote{https://pypi.org/} is the Python Package Index, a repository of software for the python programming language.
This repository helps you find and install software developed by the Python community who have decided to share their work.
The GOCPI package is distributed from this platform to enable as many as possible the ability to model their own energy systems to inform and question energy policy and investment.
Enter command: \textbf{pip install GOCPI} in the terminal to install the package using pip package management software.

\subsubsection{Code Style} \label{CS}
The GOCPI project was developed as the GOCPI package. All development code is organised within this package.
The PEP8 style for Python Code was the formatting style for development code \cite{PEP8}. 
All code was formatted with \textbf{yapf}, a formatter maintained by Google to format Python files.
Standardised formatting is important as makes the code easy to read, helps optimise the code and promotes consistency.
Docstrings and commenting were most important in documentation. A docstring is a Python inline comment. 
Each class and function has an unique docstring, a one sentence description of the function, inputs with data types and types of outputs.
The Google style docstring was most appropriate because of it's readability, ease to write and consistency with the Google Style Guide.
Additionally, automated documentation generators (\textbf{pdoc3, Sphinx} etc.) can parse this format to create documentation.
This self-consistent code style facilitated best practise maintenance and enabled reproducibility.

\subsubsection{Infrastructure}
GOCPI creates scalable energy system optimisation models with complexity size dependent.
Computations either took place locally on a 128 GB, four core Apple MacBook Pro or remotely using a cloud service. 

\subsection{Documentation}
The GOCPI project is well documented to keep an accurate record of key design decisions.
The commit history described in \ref{Version Control} was the most important form of document.
Other explicit documentation methods were applied to supplement this commit history. 
These methods, in addition to in-code documentation, include project updates and meeting minutes
nested within a project logbook.

\subsubsection{Project updates}
Project updates were recorded as itemized lists.
Each item is a brief description of the work completed during that day, week or month.
Items include, but are not limited to, completing GOCPI submodules, researching energy system statistics, building websites or writing sections of this research report.
These updates were pivotal to exploring new options, monitoring progress and making decisions to drive forward development. 
For example, the decision to adopt the OseMOSYS methodology in favour of the TIMES modelling methodology.
Project updates were transcribed to the project logbook held in this project's research compendium.

\subsubsection{Meeting minutes}
Project meetings took place for half an hour once a week. 
These meetings included discussions on energy markets, modelling methodologies, project progress and key design decisions.
The minutes from these meetings accompanies project updates in the project logbook nested within the research compendium.

\section{Results}
\begin{itemize}
	\item Langrangion
	\item KKT Theoretical
	\item Hoobyiest Edition - Mathematica
	\item Raspberry PI: 
	\item Can you predict returns, interogate returns, take inspiration from GKX, look at hedge portfolio's.
	\item Use two loss functions (MSE, Huber Loss Function), Begs question if maximise profitability
	\item Give a toy example
	\item Two period, two stocks, predict accurately but not make more money, no dials to turn, OLS susceptible to outliers, one outlier creates a massive error.
	\item OLS susceptible to outliers, windsorise (not true)
\end{itemize}
\section{Discussion}

\section{Conclusion}

\newpage
%\includepdf[pages=-]{GOCPI-appendix.pdf}
\newpage
\end{document}